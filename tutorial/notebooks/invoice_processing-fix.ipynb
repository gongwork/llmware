{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b8107d",
   "metadata": {},
   "source": [
    "https://github.com/llmware-ai/llmware/blob/main/examples/invoice_processing.py\n",
    "\n",
    "[YouTube](https://www.youtube.com/watch?v=VHZSaBBG-Bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1adb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   invoice_processing.py example illustrates the use of parsing combined with prompts_with_sources\n",
    "#   to rapidly iterate through a batch of invoices and ask a set of questions, and then save the full\n",
    "#   output to both (1) .jsonl for integration into an upstream application/database and (2) to a CSV\n",
    "#   for human review in excel.\n",
    "\n",
    "#   note: the sample code pulls from a public repo to load the sample invoice documents the first time -\n",
    "#   please feel free to substitute with your own invoice documents (PDF/DOCX/PPTX/XLSX/CSV/TXT) if you prefer.\n",
    "\n",
    "#   this example does not require a database or embedding\n",
    "\n",
    "#   this example can be run locally on a laptop by setting 'run_on_cpu=True'\n",
    "#   if 'run_on_cpu==False\", then please see the example 'launch_llmware_inference_server.py'\n",
    "#   to configure and set up a 'pop-up' GPU inference server in just a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd73dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from llmware.prompts import Prompt, HumanInTheLoop\n",
    "from llmware.configs import LLMWareConfig\n",
    "from llmware.setup import Setup\n",
    "from llmware.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f11bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoice_processing(run_on_cpu=True):\n",
    "\n",
    "    #   Step 1 - Pull down the sample files from S3 through the .load_sample_files() command\n",
    "    #   --note: if you need to refresh the sample files, set 'over_write=True'\n",
    "    print(\"update: Downloading Sample Files\")\n",
    "\n",
    "    sample_files_path = Setup().load_sample_files(over_write=False)\n",
    "    invoices_path = os.path.join(sample_files_path, \"Invoices\")\n",
    "\n",
    "    #   Step 2 - simple sample query list - each question will be asked to each invoice\n",
    "    query_list = [\"What is the total amount of the invoice?\",\n",
    "                  \"What is the invoice number?\",\n",
    "                  \"What are the names of the two parties?\"]\n",
    "\n",
    "    #   Step 3 - Load Model\n",
    "\n",
    "    if run_on_cpu:\n",
    "\n",
    "        #   load local bling model that can run on cpu/laptop\n",
    "        #   alternates to try on cpu:\n",
    "        #       -- \"llmware/bling-1.4b-0.1\", \"llmware/bling-falcon-1b-0.1\",\n",
    "        #       -- \"llmware/bling-sheared-llama-2.7b-0.1\", \"llmware/bling-sheared-llama-1.3b-0.1\",\n",
    "        #       -- \"llmware/bling-red-pajamas-3b-0.1\", \"llmware/bling-stable-lm-3b-4e1t-0.1\"\n",
    "\n",
    "        #   note: bling-1b-0.1 is the *fastest* & *smallest*, but will make more errors than larger BLING models\n",
    "        model_name = \"llmware/bling-1b-0.1\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        #   use GPU-based inference server to process\n",
    "        #  *** see the launch_llmware_inference_server.py example script to setup ***\n",
    "\n",
    "        server_uri_string = \"http://11.123.456.789:8088\"    # insert your server_uri_string\n",
    "        server_secret_key = \"demo-test\"\n",
    "        ModelCatalog().setup_custom_llmware_inference_server(server_uri_string, secret_key=server_secret_key)\n",
    "        model_name = \"llmware-inference-server\"\n",
    "\n",
    "    #   attach inference server to prompt object\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "\n",
    "    #   Step 4 - main loop thru folder of invoices\n",
    "\n",
    "    for i, invoice in enumerate(os.listdir(invoices_path)):\n",
    "\n",
    "        if i > 2: continue  # test\n",
    "            \n",
    "        #   just in case (legacy on mac os file system - not needed on linux or windows)\n",
    "        if invoice != \".DS_Store\":\n",
    "\n",
    "            print(\"\\nAnalyzing invoice: \", str(i + 1), invoice)\n",
    "\n",
    "            for question in query_list:\n",
    "\n",
    "                #   Step 4A - parses the invoices in memory and attaches as a source to the Prompt\n",
    "                source = prompter.add_source_document(invoices_path,invoice)\n",
    "\n",
    "                #   Step 4B - executes the prompt on the LLM (with the loaded source)\n",
    "                output = prompter.prompt_with_source(question,prompt_name=\"default_with_context\")\n",
    "\n",
    "                for i, response in enumerate(output):\n",
    "                    print(\"LLM Response - \", question, \" - \", re.sub(\"[\\n]\",\" \", response[\"llm_response\"]))\n",
    "\n",
    "                prompter.clear_source_materials()\n",
    "\n",
    "    # Save jsonl report with full transaction history to /prompt_history folder\n",
    "    print(\"\\nupdate: prompt state saved at: \", os.path.join(LLMWareConfig.get_prompt_path(),prompter.prompt_id))\n",
    "\n",
    "    prompter.save_state()\n",
    "\n",
    "    # Generate CSV report for easy Human review in Excel\n",
    "    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()\n",
    "\n",
    "    print(\"\\nupdate: csv output for human review - \", csv_output)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8916cb62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update: Downloading Sample Files\n",
      "\n",
      "Analyzing invoice:  2 Alpha Services Vendor Inc Invoice.pdf\n",
      "LLM Response -  What is the total amount of the invoice?  -   $22,500.00\n",
      "LLM Response -  What is the invoice number?  -   0001\n",
      "LLM Response -  What are the names of the two parties?  -   Alpha Inc. and Services Vendor Inc.\n",
      "\n",
      "Analyzing invoice:  3 Alpha Simple Invoice Template.pdf\n",
      "LLM Response -  What is the total amount of the invoice?  -   $12,980.00\n",
      "LLM Response -  What is the invoice number?  -   1.00  YOUR COMPANY.\n",
      "LLM Response -  What are the names of the two parties?  -   Alpha Inc. and Beta Inc.\n",
      "\n",
      "update: prompt state saved at:  C:\\Users\\p2p2l\\llmware_data\\prompt_history\\9e1bf68f-9400-4e71-ace0-b9b5d894215f\n",
      "\n",
      "update: csv output for human review -  {'report_name': 'interaction_report_2023-12-09_074942.csv', 'report_fp': 'C:\\\\Users\\\\p2p2l\\\\llmware_data\\\\prompt_history\\\\interaction_report_2023-12-09_074942.csv', 'results': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_processing(run_on_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e353b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef4549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e1841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308eb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5191a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
