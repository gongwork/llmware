{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184416c8",
   "metadata": {},
   "source": [
    "llmware\\examples\\llm_prompts.py\n",
    "\n",
    "\n",
    "```\n",
    "This example demonstrates:\n",
    "      1. Prompting LLMs with different kinds of sources/context\n",
    "      2. The Prompt Catalog and the use different prompt styles\n",
    "\n",
    "      Note: This example uses OpenAI's gpt-4 LLM.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e815cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.setup import Setup\n",
    "from llmware.util import PromptCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910570ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL_NAME = \"gpt-3.5-turbo\" # \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ac71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this value with your own API Key, either by setting the env var or editing it directly here:\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\",\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bd08e3f",
   "metadata": {},
   "source": [
    "openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cb8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmware provides many out of the box prompt instructions such as yes_no, number_or_none, summarize_with_bullets,etc\n",
    "def print_all_prompt_instructions():\n",
    "    print (f\"\\n > ALL AVAILABLE PROMPT INSTRUCTIONS\")\n",
    "    for prompt in PromptCatalog().get_all_prompts():\n",
    "        print (\" - \" + prompt[\"prompt_name\"])\n",
    "\n",
    "# With the provided context submit the given prompt to the LLM\n",
    "def simple_prompt_with_context_string(prompt, context, llm_name, api_key):\n",
    "    print (f\"\\n > SIMPLE PROMPT WITH CONTEXT STRING\")\n",
    "    prompter = Prompt(llm_name=llm_name, llm_api_key=api_key)\n",
    "    response = prompter.prompt_main(prompt=prompt, context=context)[\"llm_response\"]\n",
    "    print (f\"- Context: {context}\\n- Prompt: {prompt}\\n- LLM Response:\\n{response}\")\n",
    "\n",
    "# Use an llmware prompt_instruction to submit the given prompt and prompt_instruction to the LLM\n",
    "def prompt_with_prompt_instruction(prompt, context, prompt_instruction, llm_name, api_key):\n",
    "    print (f\"\\n > PROMPT WITH CONTEXT USING '{prompt_instruction}' PROMPT INSTRUCTION\")\n",
    "    prompter = Prompt(llm_name=llm_name, llm_api_key=api_key)\n",
    "    response = prompter.prompt_from_catalog(prompt=prompt, context=context, prompt_name=prompt_instruction)[\"llm_response\"]\n",
    "    print (f\"- Context: {context}\\n- Prompt: {prompt}\\n- LLM Response:\\n{response}\")\n",
    "\n",
    "\n",
    "# In some cases you may want to add additional configuraton.\n",
    "def prompt_with_inference_config(prompt, context, prompt_instruction, inference_config, llm_name, api_key):\n",
    "    print (f\"\\n > PROMPT WITH CONTEXT USING '{prompt_instruction}' PROMPT INSTRUCTION\")\n",
    "    prompter = Prompt(llm_name=llm_name, llm_api_key=api_key)\n",
    "    response = prompter.prompt_main(prompt=prompt, context=context, prompt_name=prompt_instruction, \n",
    "                                    inference_dict=inference_config)[\"llm_response\"]\n",
    "    print (f\"- Context: {context}\\n- Prompt: {prompt}\\n- LLM Response:\\n{response}\")\n",
    "\n",
    "# If the context you need to pass to an LLM is contained in Wikipedia you can easily add it as a source\n",
    "def prompt_with_wiki_source(prompt, wiki_topic, prompt_instruction, llm_name, api_key):\n",
    "    print (f\"\\n > PROMPT WITH CONTEXT FROM WIKIPEDIA USING '{prompt_instruction}' PROMPT INSTRUCTION\")\n",
    "    prompter = Prompt(llm_name=llm_name, llm_api_key=api_key)\n",
    "    prompter.add_source_wikipedia(wiki_topic, article_count=1)\n",
    "    response = prompter.prompt_with_source(prompt=prompt, prompt_name=prompt_instruction)[0][\"llm_response\"]\n",
    "    print (f\"- Context: Wikepedia article(s) for '{wiki_topic}'\\n- Prompt: {prompt}\\n- LLM Response:\\n{response}\")\n",
    "\n",
    "# If the context you need to pass is in local files, you can easily add then as sources\n",
    "def prompt_with_local_file_sources(prompt, local_folder, local_files, prompt_instruction, llm_name, api_key):\n",
    "    print (f\"\\n > PROMPT WITH CONTEXT FROM LOCAL FILE USING '{prompt_instruction}' PROMPT INSTRUCTION\")\n",
    "    prompter = Prompt(llm_name=llm_name, llm_api_key=api_key)\n",
    "    for local_file in local_files:\n",
    "        prompter.add_source_document(local_folder, local_file)\n",
    "    response = prompter.prompt_with_source(prompt=prompt, prompt_name=prompt_instruction)[0][\"llm_response\"]\n",
    "    print (f\"- Context: {local_files}\\n- Prompt: {prompt}\\n- LLM Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024aaf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > ALL AVAILABLE PROMPT INSTRUCTIONS\n",
      " - just_the_facts\n",
      " - answer_or_not_found\n",
      " - number_or_none\n",
      " - summarize_with_bullets\n",
      " - summarize_with_numbered_bullets\n",
      " - xsummary\n",
      " - completion\n",
      " - dialog_summary\n",
      " - not_found_classifier\n",
      " - top_level_select\n",
      " - answer_question_in_role\n",
      " - editor_in_role\n",
      " - yes_no\n",
      " - multiple_choice\n",
      " - default_with_context\n",
      " - default_no_context\n",
      " - summarize_with_bullets_w_query\n",
      " - summarize_with_references_w_query\n",
      " - write_poem\n",
      " - ten_words\n",
      " - explain_child\n",
      " - make_joke\n",
      " - tell_story\n",
      " - write_headline\n",
      " - facts_only\n",
      " - top_bulletpoints\n",
      " - report_title\n",
      " - marketing_slogan\n",
      " - top_level_summary\n"
     ]
    }
   ],
   "source": [
    "print_all_prompt_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df1acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > SIMPLE PROMPT WITH CONTEXT STRING\n",
      "- Context: My favorite foods are Sushi, Italian and Greek\n",
      "- Prompt: What is my 3rd favorite type of food?\n",
      "- LLM Response:\n",
      "Your 3rd favorite type of food is Greek.\n"
     ]
    }
   ],
   "source": [
    "simple_prompt_with_context_string( prompt = \"What is my 3rd favorite type of food?\",\n",
    "                                   context = \"My favorite foods are Sushi, Italian and Greek\",\n",
    "                                   llm_name = OPENAI_MODEL_NAME,\n",
    "                                   api_key = openai_api_key\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfca6abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > PROMPT WITH CONTEXT USING 'number_or_none' PROMPT INSTRUCTION\n",
      "- Context: My brother is 20 years old and my sister is 1.5 times older\n",
      "- Prompt: How old is my oldest sibling?\n",
      "- LLM Response:\n",
      "Your oldest sibling's age is not found in the given text.\n"
     ]
    }
   ],
   "source": [
    "prompt_with_prompt_instruction( prompt = \"How old is my oldest sibling?\",\n",
    "                                context = \"My brother is 20 years old and my sister is 1.5 times older\",\n",
    "                                prompt_instruction = \"number_or_none\",\n",
    "                                llm_name = OPENAI_MODEL_NAME,\n",
    "                                api_key = openai_api_key\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5311bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > PROMPT WITH CONTEXT USING 'explain_child' PROMPT INSTRUCTION\n",
      "- Context: I am interested in building rockets\n",
      "- Prompt: Why is it difficult?\n",
      "- LLM Response:\n",
      "Building rockets is difficult because it requires a lot of complex and advanced knowledge. You need to understand things like physics and engineering to be able to design and build a rocket. It also takes a lot of time and effort to gather all the materials needed and put them together in the right way. There are also many safety precautions that need to be taken into consideration. So, while building rockets can be really exciting and fun, it is also a challenging task that requires a lot of skill and hard work.\n"
     ]
    }
   ],
   "source": [
    "prompt_with_inference_config( prompt = \"Why is it difficult?\",\n",
    "                              context = \"I am interested in building rockets\",\n",
    "                              prompt_instruction = \"explain_child\",\n",
    "                              inference_config = {\"temperature\": 0.8, \"llm_max_output_len\": 1000, \"max_tokens\": 1000},\n",
    "                              llm_name = OPENAI_MODEL_NAME,\n",
    "                              api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9a2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > PROMPT WITH CONTEXT FROM WIKIPEDIA USING 'yes_no' PROMPT INSTRUCTION\n",
      "- Context: Wikepedia article(s) for 'Barack Obama'\n",
      "- Prompt: Was Barack Obama the Prime Minister of Canada?\n",
      "- LLM Response:\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "prompt_with_wiki_source( prompt = \"Was Barack Obama the Prime Minister of Canada?\",\n",
    "                         wiki_topic = \"Barack Obama\",\n",
    "                         prompt_instruction = \"yes_no\",\n",
    "                         llm_name = OPENAI_MODEL_NAME,\n",
    "                         api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23ef8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > PROMPT WITH CONTEXT FROM LOCAL FILE USING 'just_the_facts' PROMPT INSTRUCTION\n",
      "- Context: ['Gaia EXECUTIVE EMPLOYMENT AGREEMENT.pdf']\n",
      "- Prompt: What was the effective date of this agreement?\n",
      "- LLM Response:\n",
      "The effective date of this agreement is May 1, 2015.\n"
     ]
    }
   ],
   "source": [
    "prompt_with_local_file_sources( prompt = \"What was the effective date of this agreement?\",\n",
    "                                local_folder = os.path.join(Setup().load_sample_files(), \"SmallLibrary\"),\n",
    "                                local_files = ['Gaia EXECUTIVE EMPLOYMENT AGREEMENT.pdf'],\n",
    "                                prompt_instruction = \"just_the_facts\",\n",
    "                                llm_name = OPENAI_MODEL_NAME,\n",
    "                                api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdbeac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
